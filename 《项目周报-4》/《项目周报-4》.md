# 项目周报

    日期：2025-04-05

    项目实践题目：文本向量化的高级技术

## 实践内容
- 弄了一个很简单的LSH（几乎是看着AI的一点点还原的），但也花很多时间摄取了很多相关知识。如不同方向的相似度检索方法，有传统的Jaccard、Wshingling 和 Levenshtein，但这些方法比较适合检索单纯的“word”，在遇到一些意义相近而词语结构不一样，如“hi”和“hello”时表现不好。

- 再有是比较靠近向量方面的随机超平面法和simhash。我选择了随机超平面来进行主要的hash工作,随机超平面理解和实现起来都比较简单，也很适合对高维向量进行降维。

- 其他小的相关知识不再赘述

- 有了hash后，也要回归到查询上，所以我又加了一个查询方法。选择一个点之后在附近的哈希桶内寻找元素点计算向量之间的欧式距离，来选出top-k的向量。（这方面的优化也许也有内容可以探究，在我完成ColBert文章的理解后可以回头再想想）

- 用Google scholar 搜索了文章,完成了相关任务。

- 略读了推荐的《ColBert》论文，它的意思是优化了Bert的资源利用，Bert有一个注意力机制，能从左往右和从右往左来读句子。这让我想到大名鼎鼎的《Attention is all you need》但还没看，看不过来。因为ColBert提到的陌生东西有点多，一直要问AI。同时Col的later-interaction机制能优化Bert，但他优化的原理我还没理解透。

## 要做的事
- 以ColBert为接触知识的树根，遍历它伸出来的子树，但重点关注：为什么传统的BERT模型在检索任务中效率低,ColBERT的“late interaction”机制如何解决这一问题，对比“representation-focused”和“interaction-focused”模型的区别。尤其是其中的MaxSim操作，他的打分机制非常重要。




```python

```
