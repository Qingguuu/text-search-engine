# 报告提纲：
## 一、报告内容（报告内容以下条目仅供参考，请根据实际内容调整）
### 1.	项目背景与意义，需求分析与项目内容等
- 本项目的方向是：如何在海量文本向量数据集D中,快速检索文本向量q的最近似向量。在信息爆炸的时代，对信息快速检索的技术能有效提高信息资源的利用率，并大大提高工作效率。本人在经过学习文本向量的基础概念，向量检索方法如局部敏感哈希、COLBERT、HNSW等检索技术后。尝试结合聚类思想对现有的HNSW多层索引结构进行优化以提高特定工作下的检索效率。




### 2.	项目相关知识介绍

### 相似度的讨论：

- 余弦相似度 $cosθ = (A·B) / (||A||*||B||) $
- 在文本向量分析过程中，常常使用余弦相似度来衡量两个向量的相似性。因为一般的文本向量化后，向量的长度由词频决定，词频越高，它在这个维度数值越大。而且文本向量的维数一般都很高，同时也很稀疏（很多值为0）。而余弦相似度只关注方向的特性天然契合文本向量的处理。

#### 为什么余弦相似度天然适合处理文本向量？

- 假设现在有两个文本，主题都与宠物有关，前者（Q）词汇量极大，后者（P）只是一个小文本。
- 那么在经过向量化后，Q的词向量由于词频更高（比如“猫”出现了一千次），而P的词频更低（“猫”只出现了10次）。
- 如果使用欧几里得距离  $distance = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2} $来衡量，显然会出现长度相关的干扰：“猫”这个词居然和“猫”不相似？
- 而对于$cosθ = (A·B) / (||A||*||B||) $ ,我们最后计算出的是余弦值$cosθ$,它只关注向量与向量之间的“夹角”。
- 再者，文本向量都是高纬度，极度稀疏，余弦相似度通过只考虑向量点积（分子）和模长（分母），有效排除了那些同时在两个向量中都是零的维度的影响。（因为点积计算中，$0*0=0$，对结果无贡献）。分母中的模长计算也自动归一化了稀疏度的影响。它更专注于那些对两个向量同时有贡献的维度（词语）。
- .......



### 如何理解并选用文本转化为向量的技术？

#### TF-IDF 完全透明的权重分配模式，但语义欠缺........

#### W2V 训练出的语义接近，但训练过程难以解释，我们该如何解释它的神奇之处：用简单数学运算（向量加减）就能捕获语义关系，比如“国王-男人+女人≈王后”？.....

#### 实际选用场景......



### 向量从相似到选取：各类搜索技术如何“跳过”大量不可能情况？

#### HNSW等搜索技术并不对单独的归一化或是相似度比较进行优化，而是利用独特的跳转结构来实现跳过那些不可能的点，来完成迅速接近......





### 3.	对HNSW多层索引结构结合聚类思想的小尝试

#### ....









### 4.	参考文献
- [Zhang Yuge:I was trapped in Krusty Krub (just for fun)]






## 二、总结与感悟
#### 在零散的学习后，我将ANN问题从“比较距离”的最小点：相似度出发。而后从相似度的选取，不同向量化的方式，到搜索技术的讨论。在这个过程中，搜寻许多知识并对不同技术比较后。我突然明白，比较不是在进行优胜劣汰，而是为了选取特定情况下的最优答案。在比较的过程中，我们不可避免地认识并了解各种知识、技术的本质，因为只有了解本质，才能做出更好的选择。比较不是单纯的看数据，而是实现将可用放于最有用的方式之一。
















```python

```
