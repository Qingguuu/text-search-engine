# 项目周报

日期：2025-04-20

项目实践题目：文本向量化的高级技术

## 实践内容
    一、继续读SLP
    
    第9节 词向量可视化嵌入通过降维,将高维词向量投影到二维/三维空间，直观展示词语间的类别关系（如动物、职业等类别）。
    
    第10节探讨了嵌入词向量如何捕捉词语的语义关系：词向量通过高维空间中的几何关系表示语义，如“cat”和“dog”的距离更近，而且向量之间的几何运算能反映另一个词向量,如"king" - "man" + "woman" ≈ "queen"。同时，不同历史时期的文本所嵌入的同一词汇会导向不同的结果，如“gay”从“快乐的”转变为“同性恋”。这样的嵌入也许具有局限性，在表示多义词时不能准确表述。
    
    第11节 词向量的偏见问题，词向量无法自主过滤信息，因而词向量编码后可能存在社会偏见，需通过“去偏算法“”减少歧视性关联。
    
    第12节 词向量模型评估通过：外部评估：即使用 NLP 任务中的向量，并观察这是否比其他模型提高了性能。内部评估：让模型选择文本的上下文后评估相似度或进行类比任务
    第十一章中有关“上下文嵌入”的内容通过双向Transformer捕捉词汇的上下文语义,能够帮助计算语义的相似性。


    二、读了推荐的hashing的note:在文中提到，当维数过高时，若可以进行精确度上的妥协，ANN（近似最相邻）可以带来效率上的提升，关键点在于：在半径r1和r2之间寻找q的邻近点。
    
    同时文章详细讲解了LSH（局部哈希）的相关知识：它的核心思路是：让相似的点更容易被分到同一个“桶”里，而不相似的点尽量分开。
    
    他的关键点在于哈希函数的设计与选择，让相似的向量更加集中到同一个桶里，在设立不同的足够多的桶后，在搜寻向量时就不必要再去检索所有向量,而是找到那个桶后，从这个桶或相邻的桶里寻找。
    这种方法查询速度快，但也可能漏掉真正相似的点。

    
    三、完成了数学公式书写的内容熟悉,trae里面写tex公式的preview非常人性化，跟github不一样。
    
    我在上传到这里之后发现很多公式不正常显示，才发现在github提交的md文件的数学公式在$$要紧贴公式，同时y_i下标不能用{i}括起来 给我找了十分钟才找到╮(╯▽╰)╭。也算是错多学多了。

    收获：又重新略读了第六章前段，巩固了相关基础。继续往下读，了解了静态词向量的不足，延伸到动态词向量（如BERT）：基于上下文的嵌入能区分不同含义，还知道了词向量模型的评估方法。
    
    再一个又一次了解了如何处理维数爆炸的方法，理解LSH的核心思路，实现机制。再一个学会了用TEX格式书写了数学公式。

    
    
    接下来要做的内容: 上次读完的gionis论文中处理了维数爆炸，这次的相关知识也介绍如何哈希来处理维数爆炸，由此可见在NNS中，处理维数爆炸是很重要的一点。接下来可以尝试LSH代码的实现，以丰富经验的同时巩固知识。


```python

```
